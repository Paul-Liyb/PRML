\documentclass[12pt]{article}

\input{config.tex}  % 导入配置文件

\newcommand\titleofdoc{课程论文模板} % 文档标题
\newcommand\GroupName{} % 小组名

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\fancyhead[C]{\small 中国科学院大学-模式识别与机器学习小组-选题5-预研报告}

\setcounter{section}{0}


% 正文
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\begin{sloppypar}  % 两端对齐命令

 % \input{sections/cover.tex}  % 封面文件

 \input{sections/标题摘要.tex}  % 标题及摘要文件

 \vspace{0.5cm}  % 分隔 0.5cm
 
 \section{问题分析}  % 第一个section的标题（一般为引言，此处不分栏，因为根据 word 模板来看，这一行似乎是单独的，但是内容又是要分栏的，所以把第一个section的标题和内容分开了）
 	\begin{multicols*}{2}  % 正文开始分栏
 	
    \input{sections/sec1.tex}  % 第一个section的内容，接上面的标题

   
\section{相关工作调研}
在具体的调研的方法实现中，人脸特征表示，解决跨模态问题和匹配算法设计常常结合起来解决。我们一共调研了5类方法。

\subsection{CNN:VGG-Face}
使用以CNN为基础的方法，直接利用已有的VGG-Face模型,以相同的流程提取照片和漫画的特征，然后将提取出来的特征输入到传统度量学习方法（如PCA，KDA）训练的分类器中，以判断照片和漫画是否来自同一个人。由于这两种模态具有很大差异，这种不考虑照片-漫画多模态差异的迁移学习方法，在识别漫画脸部特征时效果不佳，因此对比时具有十分有限的性能。

\subsection{图片合成法}
将某一模态的图片经过转换生成得到另一模态的图片，然后在同一模态中进行匹配和识别。将人类描述的图片称为Sketch(如漫画)，则图片合成法在漫画照片人脸识别中的作用就是进行Sketch-photo转换或photo-Sketch转换，典型的工作如MRFs（如图1）和LLE。这种方法解决了上述跨模态产生差异的问题，但是其缺点在于将多模态的图片直接转化为单模态图片进行特征提取的计算量巨大。尽管可以通过采用GANs（如图2）进行优化以降低计算量，然而同样难以训练，且可解释性较差。


\begin{figure}[H]
	\centering
	\includegraphics[width=3in]{sections/figs/s-p.png}
	\caption{\label{fig2.11} \xiaowuhao \hei Sketch-photo的转换生成}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=3in]{sections/figs/gans.png}
	\caption{\label{fig2.11} \xiaowuhao \hei 利用GANs进行photo和sketch的相互转换效果图}
\end{figure}
\subsection{基于Facial landmarks的特征提取方法}
基于Facial landmarks的特征提取方法（如图3）利用
每个Facial landmark具有的视角和尺度两维参数，而其具体值由所采用的的特征提取方法决定。基于Facial landmarks的特征提取方法以固定的视角和尺度的landmark提取照片特征，以不同的视角和尺度的landmark提取漫画特征，于是每个面部地标提取到了多个跨模态指标（这是由提取漫画特征时使用了不同视角和尺度导致的）。然后利用跨模态度量学习方法（如使用距离级别的池化）来实现一个照片特征到一组漫画特征的最佳匹配，以减小照片和漫画之间视角和尺度的失调，从而达到跨模态识别的效果。
\par 下面两种特征提取方法与基于Facial landmarks的特征提取方法相结合，可以将人脸的landmark的特征信息提取，用人脸上代表性部位的特征信息对比来匹配漫画与真实图片。然而由于漫画家不同的创作风格，漫画人脸的某些特征夸张变形严重甚至出现在极不合理的位置，导致面部特征点定位困难，特征难以捕捉，这也是以下两种方法的通病。
\subsubsection{特征设计法}
特征设计法需要通过人工设计或通过学习得到在不同模态间仍然保持一致的人脸特征，同时这些特征还应满足在不同人脸间的区别度足够高，具体包括Gabor,SIFT,LBP等方法。人工设计时，这种方法的缺点在于其需要手动设计特征，虽然基于视觉神经理论，但毕竟是人为设计，难免有想当然，不妥的成分；同时在通过学习得到特征时，该方法严重依赖所给数据库，需要根据提供的数据的特点来进行设计，也就是说设计的特征不适于所有的数据集，泛化性、鲁棒性较差。当数据来源发生改变，如对RGB数据设计的特征换成了Kinect深度图像，这些特征就不一定适用，因此往往需要重新设计特征。
\subsubsection{结合CNN特征提取}
例如使用VGG-Face model，这种方法比直接利用CNN模型输出特征图的所有特征有更好的性能，比Webcaricature 只考虑单模态更能处理跨模态的表征差距，因为CNN网络强大的特征提取能力，使得这种方法性能较好。只是相对于后文提到的多任务学习方法，有些特征难以学习。

\subsection{多任务学习}
与WebCaricature等单任务学习方法相比，多任务学习方法能同时利用不同的数据训练不同的任务，如以漫画和图片作为训练数据时，多任务模型可以同时执行照片-漫画面部验证，漫画识别和照片识别，从而忽略依赖数据的噪声以学习更一般的特征。又由于该方法整合了不同的任务，所以可以学习到对于某些任务难以学习的特征。
\subsubsection{多任务学习方法}多任务学习方法分为两种：基于硬参数共享和基于软参数共享的多任务学习方法。其中，硬参数共享指的是多个任务之间共享网络的同几层隐藏层，只不过在网络的靠近输出部分开始分叉去做不同的任务；而软参数共享则是不同的任务使用不同的网络，但是不同任务的网络参数，采用距离 (L1,L2)等作为约束，鼓励参数相似化。由于不同形态的漫画和照片存在一些共同的面部特征，故使用硬参数共享的多任务学习方法是更好的选择。\subsubsection{搜索方法}多任务学习需要为每个任务设置权重，而搜索最优权重的方法主要为静态搜索与动态搜索。静态搜索中，利用实验方法手动搜索效率低，利用贪心搜索方法则费时。利用动态搜索时，若利用网络的总损失更新任务的动态权重时，容易陷入简单任务的过度训练和困难任务的不足训练。因此采用计算各个任务的损失并为具有较大损失的任务设置较大的任务权重，以重点学习困难任务。具体地，在漫画人脸照片匹配问题中，可以将漫画照片与人脸照片匹配看作主任务，将对漫画照片人物ID与真实人脸照片人物ID进行识别匹配看作另外两个子任务。将这三个任务通过最后一个共享的隐含层参数进行联系，其利用提取的任务之间的公共信息来学习任务权重。该共享隐藏层与动态权重学习模块（一个具有softmax归一化的全连接层）相连，使其生成三个任务的动态权值。该动态权重损失模块还能将各任务的损失和对应的动态权值，输入到一个新的损失函数中，以学习驱动网络专注于训练困难任务的动态权值，这样就改进了漫画人脸匹配的学习参数，使之能够有较好的效果。下图（图4）是上述自动学习生成动态权重的跨模态照片漫画识别动态多任务学习网络框架。
\end{multicols*}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{sections/figs/fl.png}
	\caption{\label{fig2.11} \xiaowuhao \hei 基于Facial landmarks的特征提取方法流程图}
\end{figure}

\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{sections/figs/cnn.png}
	\caption{\label{fig2.11} \xiaowuhao \hei 自动学习生成动态权重的跨模态照片漫画识别动态多任务学习网络框架图}
\end{figure}
\begin{multicols*}{2}
~\\~\\~\\~\\~\\~\\~\\~\\
    % \input{sections/sec2.tex}  % 第二个section（标题加内容）

    \input{sections/sec3.tex}  % 

    % ...后续section
    
    
    % 参考文献
   %  \bibliographystyle{plain}  % 参考文献的格式
   \vspace{0.5cm}
   	\nocite{*}
    \bibliography{sections/refs.bib}  % 参考文献bib文件
    \end{multicols*}
 
\end{sloppypar}
\end{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%