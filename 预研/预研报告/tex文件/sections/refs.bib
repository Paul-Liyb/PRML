@misc{huo2018webcaricature,
      title={WebCaricature: a benchmark for caricature recognition}, 
      author={Jing Huo and Wenbin Li and Yinghuan Shi and Yang Gao and Hujun Yin},
      year={2018},
      % eprint={1703.03230},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
@inproceedings{10.1145/3126686.3126736,
author = {Huo, Jing and Gao, Yang and Shi, Yinghuan and Yin, Hujun},
title = {Variation Robust Cross-Modal Metric Learning for Caricature Recognition},
year = {2017},
isbn = {9781450354165},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
% url = {https://doi.org/10.1145/3126686.3126736},
doi = {10.1145/3126686.3126736},
abstract = {In this paper, a variation robust cross-modal metric learning (VR-CM2L) method is proposed for caricature recognition. The goal of caricature recognition is to match a caricature with a photo. This recognition process needs to deal with all kind of variations including different modalities, facial appearance exaggerations, changes in viewpoint, expression, and illumination, etc. All these variations lead to severe misalignment between features of caricatures and photos. To deal with these problems, a specifically designed facial landmark based feature extraction scheme is proposed, where features of caricatures and photos are extracted using different feature extraction steps. At each facial landmark, features of photos are extracted with fixed viewing angle and scale, while features of caricatures are extracted with different scales and viewing angles. To measure the similarity of these features, multiple cross-modal metrics are learned at different facial landmarks in one optimization framework to guarantee global optimum. As the measured features are from two modalities (caricature and photo), cross-modal metric is used to remove modality variations. Pooling at distance level is used during metric optimization to further align the features of caricatures and photos. The introduced pooling step makes the learning method more robust to variations. Experimental results demonstrate the effectiveness of the proposed method on two caricature datasets with various variations.},
booktitle = {Proceedings of the on Thematic Workshops of ACM Multimedia 2017},
pages = {340–348},
numpages = {9},
keywords = {feature extraction, cross modal, metric learning, caricature recognition},
location = {Mountain View, California, USA},
series = {Thematic Workshops '17}
}
@article{10.1007/s10032-021-00364-6,
author = {Ming, Zuheng and Burie, Jean-Christophe and Luqman, Muhammad Muzzamil},
title = {Cross-Modal Photo-Caricature Face Recognition Based on Dynamic Multi-Task Learning},
year = {2021},
issue_date = {Jun 2021},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
volume = {24},
number = {1–2},
issn = {1433-2833},
% url = {https://doi.org/10.1007/s10032-021-00364-6},
doi = {10.1007/s10032-021-00364-6},
abstract = {Face recognition of realistic visual images (e.g., photos) has been well studied and made significant progress in the recent decade. However, face recognition between realistic visual images/photos and caricatures is still a challenging problem. Unlike the photos, the different artistic styles of caricatures introduce extreme non-rigid distortions of caricatures. The great representational gap between the different modalities of photos and caricatures is a big challenge for photo-caricature face recognition. In this paper, we propose to conduct cross-modal photo-caricature face recognition via multi-task learning, which can learn the features of different modalities with different tasks. Instead of manually setting the task weights as in conventional multi-task learning, this work proposes a dynamic weights learning module which can automatically generate/learn task weights according to the training importance of tasks. The learned task weights enable the network to focus on training the hard tasks instead of being stuck in the overtraining of easy tasks. The experimental results demonstrate the effectiveness of the proposed dynamic multi-task learning for cross-modal photo-caricature face recognition. The performance on the datasets CaVI and WebCaricature show the superiority over the state-of-art methods. The implementation code is provided here. ().},
journal = {Int. J. Doc. Anal. Recognit.},
month = {jun},
pages = {33–48},
numpages = {16},
keywords = {Dynamic multi-task learning, Deep CNNs, Photo-caricature face recognition}
}

